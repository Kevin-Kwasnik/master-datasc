{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0532fa5a",
   "metadata": {},
   "source": [
    "## I. Base Python Imports\n",
    "\n",
    "### 1. Reading a text file\n",
    "\n",
    "`filename = 'filename.txt'` <br>\n",
    "`file = open(filename, mode='r')` <br>\n",
    "`text = file.read()` <br>\n",
    "`file.close()` <br>\n",
    "\n",
    "In the above, variables can be defined in any way, `'filename.txt'` represents some file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b834f1",
   "metadata": {},
   "source": [
    "#### Reading a file in a context (no close required)\n",
    "\n",
    "`with open(filename, mode='r') as file:` <br>\n",
    "&emsp;&emsp; `print:file.read()` <br>\n",
    "\n",
    "What you're doing here is called 'binding' a variable in the context manager construct; while still within this construct, the variable file will be bound to open(filename, 'r'). It is best practice to use the with statement as you never have to concern yourself with closing the files again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7ee1d",
   "metadata": {},
   "source": [
    "#### Read and print individual lines\n",
    "\n",
    "`with open('moby_dick.txt') as file:` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "\n",
    "The above would read the first three lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efa470",
   "metadata": {},
   "source": [
    "### 2. Pickle\n",
    "\n",
    "`import pickle` <br>\n",
    "`with open('filename.pkl', 'rb') as file:` <br>\n",
    "&emsp;&emsp;`data = pickle.load(file)`\n",
    "\n",
    "> `'rb'` stands for read-only, binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdac69",
   "metadata": {},
   "source": [
    "## II. Numpy Imports\n",
    "\n",
    "#### 1. `np.loadtxt`\n",
    "`import numpy as np` <br>\n",
    "`filename = 'MNIST.txt`<br>\n",
    "`data = np.loadtxt(filename, delimiter=',')`<br>\n",
    "`data`\n",
    "\n",
    "**NB:** `np.loadtxt` struggles with mixed data\n",
    "\n",
    "> there are other arguments such as:\n",
    "> > `skiprows=1` <br>\n",
    "> > `usecols=[0,2]` <br>\n",
    "> > `dtype=str` (ensures all data is imported as string)\n",
    "\n",
    "#### 2. `np.genfromtxt`\n",
    "\n",
    "`data = np.genfromtxt(filename, delimiter=',', names=True, dtype=None)`\n",
    "\n",
    "`dtype=None` will figure out what types each column should be.\n",
    "`names` tells us there is a header.\n",
    "\n",
    "#### 3. `np.recfromcsv`\n",
    "\n",
    "`data = np.recfromcsv(filename)` <br>\n",
    "\n",
    "> the above operates the same was as `genfromtxt` but has defaults delimiter = ',' and names = True, as well as dtype = None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0919e58",
   "metadata": {},
   "source": [
    "## III. Pandas Imports\n",
    "\n",
    "### 1. CSV Files\n",
    "`import pandas as pd` <br>\n",
    "`filename = 'file.csv` <br>\n",
    "`data = pd.read_csv(filename)` <br>\n",
    "`data.head()`\n",
    "\n",
    "#### Customizing csv import\n",
    "`sep` = the pandas version of `delimiter` <br>\n",
    "`comment` = takes comments which occur after specified character (e.g., '#') <br>\n",
    "`na_values` = takes a list of strings to recognize as NA/NaN (e.g., 'Nothing')\n",
    "\n",
    "### 2. Excel spreadsheets\n",
    "`file = 'filename.xlsx` <br>\n",
    "`data = pd.ExcelFile(file)` <br>\n",
    "`data.sheet_names` = returns sheets in the excel file <br>\n",
    "`data.parse('sheetname')` = returns sheet specified by sheetname (as string) <br>\n",
    "`data.parse(0)` = returns sheet specified by sheetname (index position as float)\n",
    "\n",
    "#### Alternate excel import\n",
    "`data = pd.read_excel(file, sheet_name=None)`\n",
    "- `sheet_name=None` imports all sheets <br>\n",
    "`data.keys()` returns the sheetnames <br>\n",
    "`data['sheetname']` returns content from specified sheet\n",
    "\n",
    "#### Customizing excel import\n",
    "`skiprows` = select unwanted rows by passing in a list <br>\n",
    "`names` = name the columns by passing in a list (e.g., 'Country')  <br>\n",
    "`usecols` = designate which columns to parse (e.g., [0])\n",
    "\n",
    "### 3. SAS and Stata\n",
    "\n",
    "- **SAS:** Statistical Analysis System\n",
    "    - used in business analytics and biostatistics <br>\n",
    "- **Stata:** \"Statistics\" + \"data\"\n",
    "    - used in academic social science research\n",
    "    \n",
    "#### SAS files:\n",
    "- **Used for:**\n",
    "    - Advanced analytics - Multivariate analysis - Business intelligence - Data management - Predictive analytics - Standard for computational analysis \n",
    "- **Most common extensions:**\n",
    "    - `.sas7bdat` and `.sas7bcat`: dataset and catalog files, respectively.\n",
    "    \n",
    "#### Importing SAS files\n",
    "\n",
    "`import pandas as pd` <br>\n",
    "`from sas7bdat import SAS7BDAT` <br>\n",
    "`with SAS7BDAT('filename.sas7bdat') as file:` <br>\n",
    "&emsp;&emsp; `df_sas = file.to_data_frame()`\n",
    "\n",
    "#### Importing Stata files\n",
    "`import pandas as pd` <br>\n",
    "`data = pd.read_stata('filename.dta')`\n",
    "> <span style=\"color:indianred\"> no context manager (i.e., `with`) required! </span>\n",
    "\n",
    "### 4. HDF5 Files\n",
    "\n",
    "\"Hierarchical Data Format version 5\"\n",
    "- Standard for storing large quantities of numerical data\n",
    "\n",
    "\n",
    "`import h5py` <br>\n",
    "`filename = 'filename.hdf5'` <br>\n",
    "`data = h5py.File(filename, 'r')` <br>\n",
    "\n",
    "#### Exploring HDF5 files\n",
    "\n",
    "`for key in data.keys():` <br>\n",
    "&emsp;&emsp; `print(key)`\n",
    "\n",
    "- _this returns hdf groups, which can be thought of as directories_\n",
    "\n",
    "##### this continues down the structure:\n",
    "\n",
    "`for key in data['groupname'].keys():`\n",
    "&emsp;&emsp; `print(key)`\n",
    "\n",
    "- _this returns the data in the group specified in place of 'groupname'_\n",
    "\n",
    "##### to access content in the group:\n",
    "\n",
    "`print(np.array(data['groupname']['subgroupname1']), np.array(data['groupname']['subgroupname2']))`\n",
    "\n",
    "- _this converts the data to a numpy array and makes it accessible_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1457305",
   "metadata": {},
   "source": [
    "## IV. SciPy Imports\n",
    "\n",
    "### 1. MatLab Files\n",
    "\n",
    "\"Matrix Laboratory\"\n",
    "\n",
    "**read .mat files:** <br>\n",
    "`import scipy.io` <br>\n",
    "`filename = 'filename.mat'` <br>\n",
    "`mat = scipy.io.loadmat(filename)` \n",
    "\n",
    "> <span style=\"color:royalblue\"> the type of this file is a `dict` </span>\n",
    "> > <span style=\"color:royalblue\"> `keys`=MATLAB variable names </span> <br>\n",
    "<span style=\"color:royalblue\"> `values`=objects assigned to variables </span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`scipy.io.savemat()` = write .mat files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b1dee",
   "metadata": {},
   "source": [
    "## V. Creating a Database Engine with SQLAlchemy\n",
    "\n",
    "`from sqlalchemy import create_engine` <br>\n",
    "` engine = create_engine('sqlite:///Northwind.sqlite')` <br>\n",
    ">  use the function create_engine to fire up an SQL engine that will communicate our queries to the database. The only required argument of create_engine is a string that indicates the type of database you're connecting to and the name of the database. <br>\n",
    "<br>\n",
    "> <span style=\"color:indianred\"> note also: `sqlite:///` required before database name. </span>\n",
    "\n",
    "\n",
    "`table_names = engine.table_names()` <br>\n",
    "`print(table_names)`\n",
    "\n",
    "### A. Workflow of SQL querying\n",
    "\n",
    "1. import packages and functions (above, starting with `from sql...`)\n",
    "2. create the database engine (above, starting with `engine =`)\n",
    "3. connect to the engine\n",
    "4. query the database\n",
    "5. save the query results to a DataFrame\n",
    "6. close the connection\n",
    "\n",
    "#### 3. connect to engine\n",
    "\n",
    "`con = engine.connect()`\n",
    "\n",
    "#### 4. query database\n",
    "`rs = con.execute(\"SELECT * FROM Orders\")`\n",
    "\n",
    "#### 5. save query to DataFrame\n",
    "`df = pd.DataFrame(rs.fetchall())` <br>\n",
    "`df.columns = rs.keys()` \n",
    "> <span style=\"color:royalblue\"> note: this ensures the df has the proper column names </span>\n",
    "#### 6. close\n",
    "`con.close()`\n",
    "\n",
    "### B. Using Context Manager to Open Connection \n",
    "**This replaces steps 3-6 above**\n",
    "\n",
    "`with engine.connect() as con:` <br>\n",
    "&emsp;&emsp; `rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")` <br>\n",
    "&emsp;&emsp; `df = pd.DataFrame(rs.fetchmany(size=5))` <br>\n",
    "&emsp;&emsp; `df.colmns = rs.keys()`\n",
    "\n",
    "> <span style=\"color:royalblue\"> `fetchmany` returns 5 rows from the database, while `fetchall` returns all rows </span>\n",
    "\n",
    "### C. Pandas to Query\n",
    "**This also replaces steps 3-6 above**\n",
    "\n",
    "`df = pd.read_sql_query(\"SELECT * FROM Orders\", engine\")`\n",
    "\n",
    "#### Advanced Query with Pandas\n",
    "\n",
    "`df = pd.read_sql_query(\"SELECT OrderID, CompanyName FROM Orders\n",
    "INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\", engine\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0693c",
   "metadata": {},
   "source": [
    "## VI. Import Flat Files from Web\n",
    "\n",
    "### 1. urllib package\n",
    "\n",
    "- Provides interface for fetching data across the web <br>\n",
    "> `urlopen()` accepts urls instead of filenames\n",
    "\n",
    "#### save file locally & read into pd dataframe\n",
    "`from urllib.request import urlretreive` <br>\n",
    "`url = 'webaddress'` <br>\n",
    "`urlretrieve(url, 'csvname.csv')` <br>\n",
    "`df = pd.read_csv('csvname.csv', sep-';')`\n",
    "\n",
    "#### without saving locally\n",
    "`from urllib.request import urlretreive` <br>\n",
    "`url = 'webaddress'` <br>\n",
    "`df = pd.read_csv(url, sep-';')`\n",
    "\n",
    "### 2. HTTP requests to import files from web\n",
    "- protocol identifier: http: or https:\n",
    "- resource name - data.com\n",
    "            - http: \"HyperText Transfer Protocol\"\n",
    "- Goint to a website = sending HTTP request\n",
    "            - GET request\n",
    "            \n",
    "- `urlretrieve()` performs a GET request\n",
    "- HTML \"HyperText Markup Language\n",
    "\n",
    "#### a. GET request using urllib\n",
    "`from urllib.request import urlopen, Request` <br>\n",
    "`url = 'url'`  <br>\n",
    "`request = Request(url)`  <br>\n",
    "`response = urlopen(request)`  <br>\n",
    "`html = response.read()`  <br>\n",
    "`response.close()`\n",
    "\n",
    "#### b. GET requests using requests\n",
    "`import requests`  <br>\n",
    "`url = \"url\"`  <br>\n",
    "`r = requests.get(url)`  <br>\n",
    "`text = r.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b62d1",
   "metadata": {},
   "source": [
    "## VII. Web Scraping with Python\n",
    "\n",
    "**HTML:** \n",
    "- Mix of unstructured and structured data\n",
    "- structured: pre-defined data model **or** organized in defnitive manner\n",
    "- unstructured: neither of these properties\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "`from bs4 import BeautifulSoup` <br>\n",
    "`import requests`<br>\n",
    "`url = \"url\"`<br>\n",
    "`r = requests.get(url)`<br>\n",
    "`html_doc = r.text`<br>\n",
    "`soup = BeautifulSoup(html_doc)`<br>\n",
    "<br>\n",
    "#### beautiful soup methods\n",
    "`soup.prettify()`\n",
    "> <span style= \"color:royalblue\"> prettified soup is properly indented, and thus much clearer </span>\n",
    "\n",
    "`soup.title` extracts title<br>\n",
    "`soup.get_text()`extracts title and text <br> \n",
    "<br> \n",
    "`for link in soup.find_all('a'):` <br>\n",
    "&emsp;&emsp; `print(link.get('href'))` extracts all the hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b235a96",
   "metadata": {},
   "source": [
    "## VIII. APIs & JSONs\n",
    "\n",
    "API = \"Application Programming Interface\"\n",
    "\n",
    "#### JSONs\n",
    "JSON = \"JavaScript Object Notation\"\n",
    "- real-time server-to-browser communication\n",
    "- human readable\n",
    "- natural to store JSONs as a python **dict** due to key-value pairs\n",
    "- all keys are strings in JSONs, values can be otherwise\n",
    "\n",
    "### 1. Loading JSONs in Python\n",
    "`import json` <br>\n",
    "`with open('name.json', 'r') as json_file:` <br>\n",
    "&emsp;&emsp; `json_data = json.load(json_file)`\n",
    "\n",
    "> <span style= \"color:royalblue\"> note: the file type will be dict here </span>\n",
    "\n",
    "**to print the key-value pairs to the console, iterate as if dict:** <br>\n",
    "`for key, value in json_data.items():`<br>\n",
    "&emsp;&emsp; `print(key + ':' + value)`\n",
    "\n",
    "### 2. Apis and interacting with web\n",
    "\n",
    "An API is a set of protocols and routines for building and interactive with software applications.\n",
    "- Code that allows two software programs to interact with each other\n",
    "\n",
    "#### code:\n",
    "`import requests` <br>\n",
    "`url = \"apiurl\"` <br>\n",
    "`r = requests.get(url)` <br>\n",
    "`json_data = r.json()` <br>\n",
    "<br>\n",
    "`for key, value in json_data.items():` <br>\n",
    "&emsp;&emsp; `print(key + ':', value)`\n",
    "\n",
    "#### example URL:\n",
    "`http://www.omdbapi.com/?t=hackers`\n",
    "- **http** = making an http request\n",
    "- **www.omdbapi.com** = querying the omdb api\n",
    "- **?t=hackers**\n",
    "     - **?t** = Query string\n",
    "     - **=hackers** = query we are macking to the api\n",
    "     (i.e., 'return data for a move with the title 'Hackers'\n",
    "     \n",
    "### 3. Twitter API\n",
    "\n",
    "- Account required\n",
    "\n",
    "`import tweepy, json` <br>\n",
    "`access_token = \"...\"` <br>\n",
    "`access_token_secret = \"...\"` <br>\n",
    "`consumer_key = \"...\"` <br>\n",
    "`consumer_secret = \"...\"` <br>\n",
    "\n",
    "#### create a streaming object\n",
    "\n",
    "`stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)`\n",
    "\n",
    "#### filter twitter streams to capture data by keywords\n",
    "\n",
    "`stream.filter(track=['apples', 'oranges'])`\n",
    "\n",
    "#### example twitter data load:\n",
    "\n",
    "```python\n",
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "```\n",
    "\n",
    "#### turning twitter data into DataFrame\n",
    "\n",
    "```python\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "#### Twitter text analysis\n",
    "\n",
    "**function created for below analysis:**\n",
    "```python\n",
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "```\n",
    "\n",
    "**code:**\n",
    "```python\n",
    "\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc75c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
