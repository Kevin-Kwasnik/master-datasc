{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0532fa5a",
   "metadata": {},
   "source": [
    "## I. Base Python Imports\n",
    "\n",
    "### 1. Reading a text file\n",
    "\n",
    "`filename = 'filename.txt'` <br>\n",
    "`file = open(filename, mode='r')` <br>\n",
    "`text = file.read()` <br>\n",
    "`file.close()` <br>\n",
    "\n",
    "In the above, variables can be defined in any way, `'filename.txt'` represents some file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b834f1",
   "metadata": {},
   "source": [
    "#### Reading a file in a context (no close required)\n",
    "\n",
    "`with open(filename, mode='r') as file:` <br>\n",
    "&emsp;&emsp; `print:file.read()` <br>\n",
    "\n",
    "What you're doing here is called 'binding' a variable in the context manager construct; while still within this construct, the variable file will be bound to open(filename, 'r'). It is best practice to use the with statement as you never have to concern yourself with closing the files again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7ee1d",
   "metadata": {},
   "source": [
    "#### Read and print individual lines\n",
    "\n",
    "`with open('moby_dick.txt') as file:` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "&emsp;&emsp; `print(file.readline())` <br>\n",
    "\n",
    "The above would read the first three lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efa470",
   "metadata": {},
   "source": [
    "### 2. Pickle\n",
    "\n",
    "`import pickle` <br>\n",
    "`with open('filename.pkl', 'rb') as file:` <br>\n",
    "&emsp;&emsp;`data = pickle.load(file)`\n",
    "\n",
    "> `'rb'` stands for read-only, binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdac69",
   "metadata": {},
   "source": [
    "## II. Numpy Imports\n",
    "\n",
    "#### 1. `np.loadtxt`\n",
    "`import numpy as np` <br>\n",
    "`filename = 'MNIST.txt`<br>\n",
    "`data = np.loadtxt(filename, delimiter=',')`<br>\n",
    "`data`\n",
    "\n",
    "**NB:** `np.loadtxt` struggles with mixed data\n",
    "\n",
    "> there are other arguments such as:\n",
    "> > `skiprows=1` <br>\n",
    "> > `usecols=[0,2]` <br>\n",
    "> > `dtype=str` (ensures all data is imported as string)\n",
    "\n",
    "#### 2. `np.genfromtxt`\n",
    "\n",
    "`data = np.genfromtxt(filename, delimiter=',', names=True, dtype=None)`\n",
    "\n",
    "`dtype=None` will figure out what types each column should be.\n",
    "`names` tells us there is a header.\n",
    "\n",
    "#### 3. `np.recfromcsv`\n",
    "\n",
    "`data = np.recfromcsv(filename)` <br>\n",
    "\n",
    "> the above operates the same was as `genfromtxt` but has defaults delimiter = ',' and names = True, as well as dtype = None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0919e58",
   "metadata": {},
   "source": [
    "## III. Pandas Imports\n",
    "\n",
    "### 1. CSV Files\n",
    "`import pandas as pd` <br>\n",
    "`filename = 'file.csv` <br>\n",
    "`data = pd.read_csv(filename)` <br>\n",
    "`data.head()`\n",
    "\n",
    "#### Customizing csv import\n",
    "`sep` = the pandas version of `delimiter` <br>\n",
    "`comment` = takes comments which occur after specified character (e.g., '#') <br>\n",
    "`na_values` = takes a list of strings to recognize as NA/NaN (e.g., 'Nothing')\n",
    "\n",
    "### 2. Excel spreadsheets\n",
    "`file = 'filename.xlsx` <br>\n",
    "`data = pd.ExcelFile(file)` <br>\n",
    "`data.sheet_names` = returns sheets in the excel file <br>\n",
    "`data.parse('sheetname')` = returns sheet specified by sheetname (as string) <br>\n",
    "`data.parse(0)` = returns sheet specified by sheetname (index position as float)\n",
    "\n",
    "#### Alternate excel import\n",
    "`data = pd.read_excel(file, sheet_name=None)`\n",
    "- `sheet_name=None` imports all sheets <br>\n",
    "`data.keys()` returns the sheetnames <br>\n",
    "`data['sheetname']` returns content from specified sheet\n",
    "\n",
    "#### Customizing excel import\n",
    "`skiprows` = select unwanted rows by passing in a list <br>\n",
    "`names` = name the columns by passing in a list (e.g., 'Country')  <br>\n",
    "`usecols` = designate which columns to parse (e.g., [0])\n",
    "\n",
    "### 3. SAS and Stata\n",
    "\n",
    "- **SAS:** Statistical Analysis System\n",
    "    - used in business analytics and biostatistics <br>\n",
    "- **Stata:** \"Statistics\" + \"data\"\n",
    "    - used in academic social science research\n",
    "    \n",
    "#### SAS files:\n",
    "- **Used for:**\n",
    "    - Advanced analytics - Multivariate analysis - Business intelligence - Data management - Predictive analytics - Standard for computational analysis \n",
    "- **Most common extensions:**\n",
    "    - `.sas7bdat` and `.sas7bcat`: dataset and catalog files, respectively.\n",
    "    \n",
    "#### Importing SAS files\n",
    "\n",
    "`import pandas as pd` <br>\n",
    "`from sas7bdat import SAS7BDAT` <br>\n",
    "`with SAS7BDAT('filename.sas7bdat') as file:` <br>\n",
    "&emsp;&emsp; `df_sas = file.to_data_frame()`\n",
    "\n",
    "#### Importing Stata files\n",
    "`import pandas as pd` <br>\n",
    "`data = pd.read_stata('filename.dta')`\n",
    "> <span style=\"color:indianred\"> no context manager (i.e., `with`) required! </span>\n",
    "\n",
    "### 4. HDF5 Files\n",
    "\n",
    "\"Hierarchical Data Format version 5\"\n",
    "- Standard for storing large quantities of numerical data\n",
    "\n",
    "\n",
    "`import h5py` <br>\n",
    "`filename = 'filename.hdf5'` <br>\n",
    "`data = h5py.File(filename, 'r')` <br>\n",
    "\n",
    "#### Exploring HDF5 files\n",
    "\n",
    "`for key in data.keys():` <br>\n",
    "&emsp;&emsp; `print(key)`\n",
    "\n",
    "- _this returns hdf groups, which can be thought of as directories_\n",
    "\n",
    "##### this continues down the structure:\n",
    "\n",
    "`for key in data['groupname'].keys():`\n",
    "&emsp;&emsp; `print(key)`\n",
    "\n",
    "- _this returns the data in the group specified in place of 'groupname'_\n",
    "\n",
    "##### to access content in the group:\n",
    "\n",
    "`print(np.array(data['groupname']['subgroupname1']), np.array(data['groupname']['subgroupname2']))`\n",
    "\n",
    "- _this converts the data to a numpy array and makes it accessible_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1457305",
   "metadata": {},
   "source": [
    "## IV. SciPy Imports\n",
    "\n",
    "### 1. MatLab Files\n",
    "\n",
    "\"Matrix Laboratory\"\n",
    "\n",
    "**read .mat files:** <br>\n",
    "`import scipy.io` <br>\n",
    "`filename = 'filename.mat'` <br>\n",
    "`mat = scipy.io.loadmat(filename)` \n",
    "\n",
    "> <span style=\"color:royalblue\"> the type of this file is a `dict` </span>\n",
    "> > <span style=\"color:royalblue\"> `keys`=MATLAB variable names </span> <br>\n",
    "<span style=\"color:royalblue\"> `values`=objects assigned to variables </span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`scipy.io.savemat()` = write .mat files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b1dee",
   "metadata": {},
   "source": [
    "## V. Creating a Database Engine with SQLAlchemy\n",
    "\n",
    "`from sqlalchemy import create_engine` <br>\n",
    "` engine = create_engine('sqlite:///Northwind.sqlite')` <br>\n",
    ">  use the function create_engine to fire up an SQL engine that will communicate our queries to the database. The only required argument of create_engine is a string that indicates the type of database you're connecting to and the name of the database. <br>\n",
    "<br>\n",
    "> <span style=\"color:indianred\"> note also: `sqlite:///` required before database name. </span>\n",
    "\n",
    "\n",
    "`table_names = engine.table_names()` <br>\n",
    "`print(table_names)`\n",
    "\n",
    "### A. Workflow of SQL querying\n",
    "\n",
    "1. import packages and functions (above, starting with `from sql...`)\n",
    "2. create the database engine (above, starting with `engine =`)\n",
    "3. connect to the engine\n",
    "4. query the database\n",
    "5. save the query results to a DataFrame\n",
    "6. close the connection\n",
    "\n",
    "#### 3. connect to engine\n",
    "\n",
    "`con = engine.connect()`\n",
    "\n",
    "#### 4. query database\n",
    "`rs = con.execute(\"SELECT * FROM Orders\")`\n",
    "\n",
    "#### 5. save query to DataFrame\n",
    "`df = pd.DataFrame(rs.fetchall())` <br>\n",
    "`df.columns = rs.keys()` \n",
    "> <span style=\"color:royalblue\"> note: this ensures the df has the proper column names </span>\n",
    "#### 6. close\n",
    "`con.close()`\n",
    "\n",
    "### B. Using Context Manager to Open Connection \n",
    "**This replaces steps 3-6 above**\n",
    "\n",
    "`with engine.connect() as con:` <br>\n",
    "&emsp;&emsp; `rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")` <br>\n",
    "&emsp;&emsp; `df = pd.DataFrame(rs.fetchmany(size=5))` <br>\n",
    "&emsp;&emsp; `df.colmns = rs.keys()`\n",
    "\n",
    "> <span style=\"color:royalblue\"> `fetchmany` returns 5 rows from the database, while `fetchall` returns all rows </span>\n",
    "\n",
    "### C. Pandas to Query\n",
    "**This also replaces steps 3-6 above**\n",
    "\n",
    "`df = pd.read_sql_query(\"SELECT * FROM Orders\", engine\")`\n",
    "\n",
    "#### Advanced Query with Pandas\n",
    "\n",
    "`df = pd.read_sql_query(\"SELECT OrderID, CompanyName FROM Orders\n",
    "INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\", engine\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75547f",
   "metadata": {},
   "source": [
    "## VI. Import Flat Files from Web\n",
    "\n",
    "### 1. urllib package\n",
    "\n",
    "- Provides interface for fetching data across the web <br>\n",
    "> `urlopen()` accepts urls instead of filenames\n",
    "\n",
    "#### save file locally & read into pd dataframe\n",
    "`from urllib.request import urlretreive` <br>\n",
    "`url = 'webaddress'` <br>\n",
    "`urlretrieve(url, 'csvname.csv')` <br>\n",
    "`df = pd.read_csv('csvname.csv', sep-';')`\n",
    "\n",
    "#### without saving locally\n",
    "`from urllib.request import urlretreive` <br>\n",
    "`url = 'webaddress'` <br>\n",
    "`df = pd.read_csv(url, sep-';')`\n",
    "\n",
    "### 2. HTTP requests to import files from web\n",
    "- protocol identifier: http: or https:\n",
    "- resource name - data.com\n",
    "            - http: \"HyperText Transfer Protocol\"\n",
    "- Goint to a website = sending HTTP request\n",
    "            - GET request\n",
    "            \n",
    "- `urlretrieve()` performs a GET request\n",
    "- HTML \"HyperText Markup Language\n",
    "\n",
    "#### a. GET request using urllib\n",
    "`from urllib.request import urlopen, Request` <br>\n",
    "`url = 'url'`  <br>\n",
    "`request = Request(url)`  <br>\n",
    "`response = urlopen(request)`  <br>\n",
    "`html = response.read()`  <br>\n",
    "`response.close()`\n",
    "\n",
    "#### b. GET requests using requests\n",
    "`import requests`  <br>\n",
    "`url = \"url\"`  <br>\n",
    "`r = requests.get(url)`  <br>\n",
    "`text = r.text`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb45ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
