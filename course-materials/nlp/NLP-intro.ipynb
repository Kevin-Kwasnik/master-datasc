{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31496ad1",
   "metadata": {},
   "source": [
    "## I. Regular Expressions (Regex)\n",
    "\n",
    "- Strings with a special syntax \n",
    "- Allow us to match patterns in other strings\n",
    "- Applications:\n",
    "    - find weblinks in a document\n",
    "    - parse email addresses\n",
    "    - remove/replace unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c12441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11173153",
   "metadata": {},
   "source": [
    "### 1. Reference\n",
    "\n",
    "|pattern | matches | example |\n",
    "| :- | :- | :- |\n",
    "| \\w+| word | 'Magic' |\n",
    "| \\d | digit | 9 |\n",
    "| \\s | space | '' |\n",
    "| .* | wildcard | 'username74' |\n",
    "| + or * | greedy match | 'aaaaaa' |\n",
    "| \\S| **not** space | 'no_spaces' |\n",
    "| [a-z]| lowercase group | 'abcdefg' |\n",
    "| [A-Z]| uppercase group | 'ABCDEFG' |\n",
    "| [.?!]| symbol group | '.' or '?'|\n",
    "|[A-Za-a] | upper and lowercase English alphabet | \"ABCDEFghijk\" |\n",
    "| [0-9] | numbers from 0 to 9 | 9 |\n",
    "| [A-Za-z\\-\\.] | upper and lowercase English alphabet, - and . | 'My-Website.com' |\n",
    "| (a-z) | a, - and z | 'a-z' |\n",
    "| (\\s+|,) | spaces or a comma | ',' |\n",
    "\n",
    "> **note**: since `-` and `.` are special characters in regex, to look for them explicitly an escape character `\\` is needed directly before the character. \n",
    "\n",
    "**example** find anything in square brackets:\n",
    "```python\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "```\n",
    "#### Regex using or \"I\"\n",
    "- OR is represented using `|`\n",
    "- You can define a group using `()`\n",
    "> only what is defined explicitly is matched\n",
    "- You can define explicit character ranges using `[]`\n",
    "\n",
    "**example** find any words or digits:\n",
    "```python\n",
    "match_digits_and_words = r\"(\\d+|\\w+)\"\n",
    "```\n",
    "### 2. re methods\n",
    "- <span style= \"color:indianred\">Pattern first, string second </span>\n",
    "- May return an iterator, string, or match object\n",
    "\n",
    "#### match()\n",
    "- matches a pattern with a string, taking pattern as first arg and string as second and returns match oobject.\n",
    "\n",
    "- note: using symbols as capital negates them\n",
    "\n",
    "#### search()\n",
    "- search for a pattern\n",
    "\n",
    "> <span style= \"color:royalblue\"> **NOTE** on search vs. match: `search` will go through the ENTIRE string to look for match options, while `match` tries to match from the beginning of a string until it can no longer match.</span> <br>\n",
    "<span style= \"color:indianred\"> If you need to find a pattern that might not be at the beginning of the string, you should use search. If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use match.</span>\n",
    "\n",
    "#### split()\n",
    "- split a string on a regex\n",
    "\n",
    "**e.g.:**\n",
    "```python\n",
    "re.split('\\s+', 'Split on spaces.')\n",
    "# would return:\n",
    "['Split', 'on', 'spaces.']\n",
    "```\n",
    "> *This can be used for tokenization, so you can preprocess text using regex*\n",
    "\n",
    "#### findall()\n",
    "- find all patterns in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8449ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE:\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73e8eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978415",
   "metadata": {},
   "source": [
    "## II. Tokenization\n",
    "- <span style= \"color:indianred\">Pattern first, string second </span>\n",
    "\n",
    "#### overview\n",
    "- Transforming a string or document into tokens(smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- Many different theories and rules\n",
    "- You can create your own rules using regular expressions\n",
    "- Examples:\n",
    "    - breaking out words or sentences\n",
    "    - separating punctuation\n",
    "    - spearating parts, such as all hashtags in a tweet\n",
    "  \n",
    "#### value of tokenization\n",
    "- Easier to match part of speech\n",
    "- Matching common words\n",
    "- Removing unwanted tokens\n",
    "> e.g.: \"I don't like Sam's shoes.\" <br>\n",
    "reveals **negation** in `\"n't\"` and **possession** in `\"'s\"` in the following output: <br>\n",
    "```python\n",
    "[\"I\", \"do\",\"n't\",\"like\",\"Sam\",\"'s\",\"shoes\",\".\"]\n",
    "```\n",
    "  \n",
    "### nltk library\n",
    "`nltk`: natural language toolkit\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "work_tokenize(\"Hi there!\")\n",
    "```\n",
    "would output: \n",
    "```python\n",
    "['Hi', 'there', '!']\n",
    "```\n",
    "\n",
    "#### other nltk tokenizers\n",
    "\n",
    "`sent_tokenize`: tokenize a document into sentences <br>\n",
    "`regexp_tokenize`: tokenize a string or document based on a regular expression pattern <br>\n",
    "`TweetTokenizer`: special class for tweet tokenization, allows separation of hashtags, mentions and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e40ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba81c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd405b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_string, r\"(\\w+|#\\d|\\?|!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c53f1d",
   "metadata": {},
   "source": [
    "#### how to find mentions and hashtags:\n",
    "```python\n",
    "r\"([@#]\\w+)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474a25d",
   "metadata": {},
   "source": [
    "#### How to tokenize tweets\n",
    "```python\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781f4b5",
   "metadata": {},
   "source": [
    "### III. Bag-of-Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e90a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
